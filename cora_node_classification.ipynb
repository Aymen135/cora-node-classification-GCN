{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"./data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.max().item() +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[  2, 410, 471, 552, 565]]),\n",
       "       values=tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]),\n",
       "       size=(2708,), nnz=5, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.4103 acc_train: 0.9357 loss_val: 0.6875 acc_val: 0.8133 time: 0.0180s\n",
      "Epoch: 0002 loss_train: 0.3487 acc_train: 0.9500 loss_val: 0.6858 acc_val: 0.8133 time: 0.0160s\n",
      "Epoch: 0003 loss_train: 0.3739 acc_train: 0.9643 loss_val: 0.6835 acc_val: 0.8133 time: 0.0170s\n",
      "Epoch: 0004 loss_train: 0.4010 acc_train: 0.9429 loss_val: 0.6803 acc_val: 0.8167 time: 0.0150s\n",
      "Epoch: 0005 loss_train: 0.3601 acc_train: 0.9714 loss_val: 0.6775 acc_val: 0.8133 time: 0.0160s\n",
      "Epoch: 0006 loss_train: 0.3812 acc_train: 0.9286 loss_val: 0.6744 acc_val: 0.8100 time: 0.0160s\n",
      "Epoch: 0007 loss_train: 0.3972 acc_train: 0.9500 loss_val: 0.6724 acc_val: 0.8100 time: 0.0160s\n",
      "Epoch: 0008 loss_train: 0.3864 acc_train: 0.9500 loss_val: 0.6727 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0009 loss_train: 0.3912 acc_train: 0.9571 loss_val: 0.6747 acc_val: 0.8200 time: 0.0150s\n",
      "Epoch: 0010 loss_train: 0.3869 acc_train: 0.9143 loss_val: 0.6750 acc_val: 0.8200 time: 0.0160s\n",
      "Epoch: 0011 loss_train: 0.3964 acc_train: 0.9429 loss_val: 0.6755 acc_val: 0.8200 time: 0.0150s\n",
      "Epoch: 0012 loss_train: 0.3861 acc_train: 0.9643 loss_val: 0.6740 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0013 loss_train: 0.4059 acc_train: 0.9286 loss_val: 0.6736 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0014 loss_train: 0.3467 acc_train: 0.9643 loss_val: 0.6705 acc_val: 0.8133 time: 0.0140s\n",
      "Epoch: 0015 loss_train: 0.3755 acc_train: 0.9214 loss_val: 0.6666 acc_val: 0.8167 time: 0.0160s\n",
      "Epoch: 0016 loss_train: 0.3654 acc_train: 0.9500 loss_val: 0.6638 acc_val: 0.8167 time: 0.0160s\n",
      "Epoch: 0017 loss_train: 0.3319 acc_train: 0.9357 loss_val: 0.6620 acc_val: 0.8200 time: 0.0160s\n",
      "Epoch: 0018 loss_train: 0.3680 acc_train: 0.9643 loss_val: 0.6614 acc_val: 0.8200 time: 0.0160s\n",
      "Epoch: 0019 loss_train: 0.3389 acc_train: 0.9500 loss_val: 0.6621 acc_val: 0.8167 time: 0.0140s\n",
      "Epoch: 0020 loss_train: 0.3623 acc_train: 0.9429 loss_val: 0.6627 acc_val: 0.8200 time: 0.0150s\n",
      "Epoch: 0021 loss_train: 0.3194 acc_train: 0.9571 loss_val: 0.6640 acc_val: 0.8200 time: 0.0150s\n",
      "Epoch: 0022 loss_train: 0.3373 acc_train: 0.9714 loss_val: 0.6643 acc_val: 0.8167 time: 0.0160s\n",
      "Epoch: 0023 loss_train: 0.3459 acc_train: 0.9643 loss_val: 0.6642 acc_val: 0.8167 time: 0.0180s\n",
      "Epoch: 0024 loss_train: 0.3562 acc_train: 0.9571 loss_val: 0.6654 acc_val: 0.8167 time: 0.0160s\n",
      "Epoch: 0025 loss_train: 0.3793 acc_train: 0.9500 loss_val: 0.6659 acc_val: 0.8200 time: 0.0160s\n",
      "Epoch: 0026 loss_train: 0.3673 acc_train: 0.9571 loss_val: 0.6671 acc_val: 0.8167 time: 0.0160s\n",
      "Epoch: 0027 loss_train: 0.3020 acc_train: 0.9786 loss_val: 0.6653 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0028 loss_train: 0.3827 acc_train: 0.9357 loss_val: 0.6646 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0029 loss_train: 0.3562 acc_train: 0.9714 loss_val: 0.6628 acc_val: 0.8200 time: 0.0140s\n",
      "Epoch: 0030 loss_train: 0.3721 acc_train: 0.9500 loss_val: 0.6620 acc_val: 0.8167 time: 0.0160s\n",
      "Epoch: 0031 loss_train: 0.3602 acc_train: 0.9429 loss_val: 0.6607 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0032 loss_train: 0.3272 acc_train: 0.9714 loss_val: 0.6612 acc_val: 0.8133 time: 0.0140s\n",
      "Epoch: 0033 loss_train: 0.3247 acc_train: 0.9500 loss_val: 0.6616 acc_val: 0.8133 time: 0.0155s\n",
      "Epoch: 0034 loss_train: 0.3719 acc_train: 0.9357 loss_val: 0.6631 acc_val: 0.8100 time: 0.0140s\n",
      "Epoch: 0035 loss_train: 0.3275 acc_train: 0.9714 loss_val: 0.6642 acc_val: 0.8133 time: 0.0140s\n",
      "Epoch: 0036 loss_train: 0.3318 acc_train: 0.9500 loss_val: 0.6662 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0037 loss_train: 0.3729 acc_train: 0.9571 loss_val: 0.6651 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0038 loss_train: 0.3264 acc_train: 0.9714 loss_val: 0.6639 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0039 loss_train: 0.3223 acc_train: 0.9929 loss_val: 0.6604 acc_val: 0.8167 time: 0.0150s\n",
      "Epoch: 0040 loss_train: 0.3091 acc_train: 0.9500 loss_val: 0.6578 acc_val: 0.8167 time: 0.0170s\n",
      "Epoch: 0041 loss_train: 0.3527 acc_train: 0.9500 loss_val: 0.6571 acc_val: 0.8167 time: 0.0140s\n",
      "Epoch: 0042 loss_train: 0.3476 acc_train: 0.9571 loss_val: 0.6585 acc_val: 0.8200 time: 0.0150s\n",
      "Epoch: 0043 loss_train: 0.3839 acc_train: 0.9500 loss_val: 0.6608 acc_val: 0.8133 time: 0.0120s\n",
      "Epoch: 0044 loss_train: 0.3272 acc_train: 0.9643 loss_val: 0.6650 acc_val: 0.8067 time: 0.0140s\n",
      "Epoch: 0045 loss_train: 0.3366 acc_train: 0.9500 loss_val: 0.6677 acc_val: 0.8067 time: 0.0160s\n",
      "Epoch: 0046 loss_train: 0.3511 acc_train: 0.9500 loss_val: 0.6675 acc_val: 0.8033 time: 0.0160s\n",
      "Epoch: 0047 loss_train: 0.3476 acc_train: 0.9786 loss_val: 0.6655 acc_val: 0.8067 time: 0.0150s\n",
      "Epoch: 0048 loss_train: 0.3419 acc_train: 0.9286 loss_val: 0.6603 acc_val: 0.8100 time: 0.0130s\n",
      "Epoch: 0049 loss_train: 0.3200 acc_train: 0.9643 loss_val: 0.6538 acc_val: 0.8200 time: 0.0150s\n",
      "Epoch: 0050 loss_train: 0.3148 acc_train: 0.9643 loss_val: 0.6500 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0051 loss_train: 0.3531 acc_train: 0.9429 loss_val: 0.6479 acc_val: 0.8200 time: 0.0150s\n",
      "Epoch: 0052 loss_train: 0.3381 acc_train: 0.9786 loss_val: 0.6476 acc_val: 0.8200 time: 0.0150s\n",
      "Epoch: 0053 loss_train: 0.3522 acc_train: 0.9357 loss_val: 0.6487 acc_val: 0.8200 time: 0.0130s\n",
      "Epoch: 0054 loss_train: 0.3797 acc_train: 0.9214 loss_val: 0.6506 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0055 loss_train: 0.3251 acc_train: 0.9643 loss_val: 0.6536 acc_val: 0.8100 time: 0.0160s\n",
      "Epoch: 0056 loss_train: 0.3275 acc_train: 0.9571 loss_val: 0.6576 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0057 loss_train: 0.3476 acc_train: 0.9714 loss_val: 0.6603 acc_val: 0.8100 time: 0.0130s\n",
      "Epoch: 0058 loss_train: 0.3707 acc_train: 0.9214 loss_val: 0.6612 acc_val: 0.8133 time: 0.0160s\n",
      "Epoch: 0059 loss_train: 0.3227 acc_train: 0.9429 loss_val: 0.6590 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0060 loss_train: 0.3024 acc_train: 0.9643 loss_val: 0.6535 acc_val: 0.8133 time: 0.0160s\n",
      "Epoch: 0061 loss_train: 0.3193 acc_train: 0.9643 loss_val: 0.6484 acc_val: 0.8200 time: 0.0150s\n",
      "Epoch: 0062 loss_train: 0.3221 acc_train: 0.9500 loss_val: 0.6461 acc_val: 0.8167 time: 0.0160s\n",
      "Epoch: 0063 loss_train: 0.2831 acc_train: 0.9857 loss_val: 0.6445 acc_val: 0.8133 time: 0.0140s\n",
      "Epoch: 0064 loss_train: 0.2925 acc_train: 0.9643 loss_val: 0.6442 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0065 loss_train: 0.2939 acc_train: 0.9643 loss_val: 0.6437 acc_val: 0.8167 time: 0.0150s\n",
      "Epoch: 0066 loss_train: 0.3572 acc_train: 0.9429 loss_val: 0.6455 acc_val: 0.8200 time: 0.0150s\n",
      "Epoch: 0067 loss_train: 0.3348 acc_train: 0.9643 loss_val: 0.6491 acc_val: 0.8133 time: 0.0160s\n",
      "Epoch: 0068 loss_train: 0.2898 acc_train: 0.9714 loss_val: 0.6523 acc_val: 0.8067 time: 0.0150s\n",
      "Epoch: 0069 loss_train: 0.3122 acc_train: 0.9714 loss_val: 0.6545 acc_val: 0.8067 time: 0.0140s\n",
      "Epoch: 0070 loss_train: 0.3246 acc_train: 0.9500 loss_val: 0.6575 acc_val: 0.8067 time: 0.0150s\n",
      "Epoch: 0071 loss_train: 0.3027 acc_train: 0.9714 loss_val: 0.6584 acc_val: 0.8067 time: 0.0140s\n",
      "Epoch: 0072 loss_train: 0.3060 acc_train: 0.9714 loss_val: 0.6553 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0073 loss_train: 0.3380 acc_train: 0.9357 loss_val: 0.6500 acc_val: 0.8167 time: 0.0450s\n",
      "Epoch: 0074 loss_train: 0.3343 acc_train: 0.9571 loss_val: 0.6461 acc_val: 0.8167 time: 0.0240s\n",
      "Epoch: 0075 loss_train: 0.3260 acc_train: 0.9571 loss_val: 0.6436 acc_val: 0.8200 time: 0.0170s\n",
      "Epoch: 0076 loss_train: 0.3387 acc_train: 0.9714 loss_val: 0.6426 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0077 loss_train: 0.3178 acc_train: 0.9714 loss_val: 0.6424 acc_val: 0.8133 time: 0.0140s\n",
      "Epoch: 0078 loss_train: 0.3408 acc_train: 0.9571 loss_val: 0.6428 acc_val: 0.8200 time: 0.0140s\n",
      "Epoch: 0079 loss_train: 0.3250 acc_train: 0.9643 loss_val: 0.6450 acc_val: 0.8167 time: 0.0140s\n",
      "Epoch: 0080 loss_train: 0.3306 acc_train: 0.9643 loss_val: 0.6466 acc_val: 0.8133 time: 0.0140s\n",
      "Epoch: 0081 loss_train: 0.2935 acc_train: 0.9429 loss_val: 0.6494 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0082 loss_train: 0.3449 acc_train: 0.9286 loss_val: 0.6524 acc_val: 0.8133 time: 0.0160s\n",
      "Epoch: 0083 loss_train: 0.3149 acc_train: 0.9429 loss_val: 0.6547 acc_val: 0.8067 time: 0.0150s\n",
      "Epoch: 0084 loss_train: 0.2930 acc_train: 0.9714 loss_val: 0.6538 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0085 loss_train: 0.2786 acc_train: 0.9929 loss_val: 0.6505 acc_val: 0.8167 time: 0.0160s\n",
      "Epoch: 0086 loss_train: 0.2785 acc_train: 0.9857 loss_val: 0.6452 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0087 loss_train: 0.2857 acc_train: 0.9714 loss_val: 0.6410 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0088 loss_train: 0.3394 acc_train: 0.9500 loss_val: 0.6394 acc_val: 0.8167 time: 0.0160s\n",
      "Epoch: 0089 loss_train: 0.2847 acc_train: 0.9571 loss_val: 0.6390 acc_val: 0.8133 time: 0.0140s\n",
      "Epoch: 0090 loss_train: 0.2987 acc_train: 0.9714 loss_val: 0.6395 acc_val: 0.8167 time: 0.0160s\n",
      "Epoch: 0091 loss_train: 0.3009 acc_train: 0.9786 loss_val: 0.6422 acc_val: 0.8167 time: 0.0160s\n",
      "Epoch: 0092 loss_train: 0.3077 acc_train: 0.9643 loss_val: 0.6449 acc_val: 0.8133 time: 0.0160s\n",
      "Epoch: 0093 loss_train: 0.3208 acc_train: 0.9429 loss_val: 0.6456 acc_val: 0.8033 time: 0.0150s\n",
      "Epoch: 0094 loss_train: 0.3027 acc_train: 0.9714 loss_val: 0.6448 acc_val: 0.8033 time: 0.0150s\n",
      "Epoch: 0095 loss_train: 0.2786 acc_train: 0.9786 loss_val: 0.6456 acc_val: 0.8067 time: 0.0140s\n",
      "Epoch: 0096 loss_train: 0.3313 acc_train: 0.9357 loss_val: 0.6477 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0097 loss_train: 0.3063 acc_train: 0.9643 loss_val: 0.6496 acc_val: 0.8133 time: 0.0146s\n",
      "Epoch: 0098 loss_train: 0.3153 acc_train: 0.9500 loss_val: 0.6491 acc_val: 0.8100 time: 0.0130s\n",
      "Epoch: 0099 loss_train: 0.2688 acc_train: 0.9571 loss_val: 0.6474 acc_val: 0.8133 time: 0.0160s\n",
      "Epoch: 0100 loss_train: 0.2764 acc_train: 0.9929 loss_val: 0.6450 acc_val: 0.8167 time: 0.0150s\n",
      "Epoch: 0101 loss_train: 0.3217 acc_train: 0.9571 loss_val: 0.6439 acc_val: 0.8167 time: 0.0150s\n",
      "Epoch: 0102 loss_train: 0.2987 acc_train: 0.9643 loss_val: 0.6404 acc_val: 0.8167 time: 0.0150s\n",
      "Epoch: 0103 loss_train: 0.3340 acc_train: 0.9429 loss_val: 0.6390 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0104 loss_train: 0.2925 acc_train: 0.9714 loss_val: 0.6393 acc_val: 0.8167 time: 0.0150s\n",
      "Epoch: 0105 loss_train: 0.2617 acc_train: 0.9714 loss_val: 0.6418 acc_val: 0.8067 time: 0.0140s\n",
      "Epoch: 0106 loss_train: 0.2664 acc_train: 0.9786 loss_val: 0.6436 acc_val: 0.8067 time: 0.0150s\n",
      "Epoch: 0107 loss_train: 0.2829 acc_train: 0.9714 loss_val: 0.6436 acc_val: 0.8067 time: 0.0150s\n",
      "Epoch: 0108 loss_train: 0.2980 acc_train: 0.9714 loss_val: 0.6415 acc_val: 0.8033 time: 0.0150s\n",
      "Epoch: 0109 loss_train: 0.2889 acc_train: 0.9571 loss_val: 0.6383 acc_val: 0.8100 time: 0.0140s\n",
      "Epoch: 0110 loss_train: 0.3014 acc_train: 0.9571 loss_val: 0.6357 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0111 loss_train: 0.2729 acc_train: 0.9571 loss_val: 0.6346 acc_val: 0.8100 time: 0.0140s\n",
      "Epoch: 0112 loss_train: 0.3081 acc_train: 0.9571 loss_val: 0.6344 acc_val: 0.8167 time: 0.0150s\n",
      "Epoch: 0113 loss_train: 0.3081 acc_train: 0.9571 loss_val: 0.6343 acc_val: 0.8167 time: 0.0160s\n",
      "Epoch: 0114 loss_train: 0.2869 acc_train: 0.9714 loss_val: 0.6364 acc_val: 0.8167 time: 0.0150s\n",
      "Epoch: 0115 loss_train: 0.2635 acc_train: 0.9786 loss_val: 0.6396 acc_val: 0.8200 time: 0.0140s\n",
      "Epoch: 0116 loss_train: 0.2697 acc_train: 0.9643 loss_val: 0.6396 acc_val: 0.8100 time: 0.0160s\n",
      "Epoch: 0117 loss_train: 0.2761 acc_train: 0.9643 loss_val: 0.6382 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0118 loss_train: 0.2887 acc_train: 0.9714 loss_val: 0.6350 acc_val: 0.8067 time: 0.0140s\n",
      "Epoch: 0119 loss_train: 0.2846 acc_train: 0.9714 loss_val: 0.6327 acc_val: 0.8033 time: 0.0150s\n",
      "Epoch: 0120 loss_train: 0.2869 acc_train: 0.9643 loss_val: 0.6309 acc_val: 0.8100 time: 0.0140s\n",
      "Epoch: 0121 loss_train: 0.2668 acc_train: 0.9643 loss_val: 0.6286 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0122 loss_train: 0.2960 acc_train: 0.9643 loss_val: 0.6281 acc_val: 0.8167 time: 0.0150s\n",
      "Epoch: 0123 loss_train: 0.2463 acc_train: 0.9571 loss_val: 0.6279 acc_val: 0.8200 time: 0.0150s\n",
      "Epoch: 0124 loss_train: 0.3047 acc_train: 0.9571 loss_val: 0.6286 acc_val: 0.8200 time: 0.0150s\n",
      "Epoch: 0125 loss_train: 0.2989 acc_train: 0.9643 loss_val: 0.6300 acc_val: 0.8167 time: 0.0150s\n",
      "Epoch: 0126 loss_train: 0.2823 acc_train: 0.9714 loss_val: 0.6313 acc_val: 0.8100 time: 0.0140s\n",
      "Epoch: 0127 loss_train: 0.2892 acc_train: 0.9643 loss_val: 0.6340 acc_val: 0.8033 time: 0.0160s\n",
      "Epoch: 0128 loss_train: 0.2932 acc_train: 0.9500 loss_val: 0.6344 acc_val: 0.8000 time: 0.0160s\n",
      "Epoch: 0129 loss_train: 0.2161 acc_train: 0.9857 loss_val: 0.6346 acc_val: 0.8000 time: 0.0160s\n",
      "Epoch: 0130 loss_train: 0.2471 acc_train: 0.9714 loss_val: 0.6353 acc_val: 0.8033 time: 0.0160s\n",
      "Epoch: 0131 loss_train: 0.2680 acc_train: 0.9714 loss_val: 0.6369 acc_val: 0.7967 time: 0.0150s\n",
      "Epoch: 0132 loss_train: 0.3266 acc_train: 0.9571 loss_val: 0.6366 acc_val: 0.8000 time: 0.0130s\n",
      "Epoch: 0133 loss_train: 0.2525 acc_train: 0.9714 loss_val: 0.6361 acc_val: 0.8000 time: 0.0150s\n",
      "Epoch: 0134 loss_train: 0.2819 acc_train: 0.9714 loss_val: 0.6342 acc_val: 0.8067 time: 0.0140s\n",
      "Epoch: 0135 loss_train: 0.3050 acc_train: 0.9714 loss_val: 0.6319 acc_val: 0.8033 time: 0.0140s\n",
      "Epoch: 0136 loss_train: 0.3074 acc_train: 0.9571 loss_val: 0.6314 acc_val: 0.8067 time: 0.0150s\n",
      "Epoch: 0137 loss_train: 0.2737 acc_train: 0.9643 loss_val: 0.6314 acc_val: 0.8033 time: 0.0150s\n",
      "Epoch: 0138 loss_train: 0.2952 acc_train: 0.9714 loss_val: 0.6316 acc_val: 0.8033 time: 0.0140s\n",
      "Epoch: 0139 loss_train: 0.2951 acc_train: 0.9714 loss_val: 0.6319 acc_val: 0.8033 time: 0.0140s\n",
      "Epoch: 0140 loss_train: 0.2645 acc_train: 0.9857 loss_val: 0.6331 acc_val: 0.7967 time: 0.0150s\n",
      "Epoch: 0141 loss_train: 0.3097 acc_train: 0.9643 loss_val: 0.6324 acc_val: 0.8000 time: 0.0150s\n",
      "Epoch: 0142 loss_train: 0.3203 acc_train: 0.9571 loss_val: 0.6307 acc_val: 0.8067 time: 0.0140s\n",
      "Epoch: 0143 loss_train: 0.2458 acc_train: 0.9786 loss_val: 0.6318 acc_val: 0.8067 time: 0.0150s\n",
      "Epoch: 0144 loss_train: 0.2767 acc_train: 0.9500 loss_val: 0.6343 acc_val: 0.8033 time: 0.0150s\n",
      "Epoch: 0145 loss_train: 0.2699 acc_train: 0.9714 loss_val: 0.6374 acc_val: 0.8000 time: 0.0180s\n",
      "Epoch: 0146 loss_train: 0.2754 acc_train: 0.9714 loss_val: 0.6409 acc_val: 0.8033 time: 0.0150s\n",
      "Epoch: 0147 loss_train: 0.2899 acc_train: 0.9571 loss_val: 0.6418 acc_val: 0.8000 time: 0.0140s\n",
      "Epoch: 0148 loss_train: 0.2739 acc_train: 0.9786 loss_val: 0.6406 acc_val: 0.8000 time: 0.0160s\n",
      "Epoch: 0149 loss_train: 0.2898 acc_train: 0.9857 loss_val: 0.6359 acc_val: 0.8000 time: 0.0150s\n",
      "Epoch: 0150 loss_train: 0.2574 acc_train: 0.9714 loss_val: 0.6336 acc_val: 0.8000 time: 0.0130s\n",
      "Epoch: 0151 loss_train: 0.2710 acc_train: 0.9786 loss_val: 0.6331 acc_val: 0.8000 time: 0.0150s\n",
      "Epoch: 0152 loss_train: 0.2733 acc_train: 0.9714 loss_val: 0.6315 acc_val: 0.8000 time: 0.0160s\n",
      "Epoch: 0153 loss_train: 0.2861 acc_train: 0.9786 loss_val: 0.6297 acc_val: 0.8033 time: 0.0160s\n",
      "Epoch: 0154 loss_train: 0.2741 acc_train: 0.9714 loss_val: 0.6302 acc_val: 0.8033 time: 0.0160s\n",
      "Epoch: 0155 loss_train: 0.2635 acc_train: 0.9643 loss_val: 0.6316 acc_val: 0.8033 time: 0.0140s\n",
      "Epoch: 0156 loss_train: 0.2411 acc_train: 0.9714 loss_val: 0.6335 acc_val: 0.8133 time: 0.0140s\n",
      "Epoch: 0157 loss_train: 0.2595 acc_train: 0.9714 loss_val: 0.6363 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0158 loss_train: 0.2350 acc_train: 0.9786 loss_val: 0.6386 acc_val: 0.8067 time: 0.0160s\n",
      "Epoch: 0159 loss_train: 0.2556 acc_train: 0.9500 loss_val: 0.6362 acc_val: 0.7967 time: 0.0140s\n",
      "Epoch: 0160 loss_train: 0.2946 acc_train: 0.9643 loss_val: 0.6344 acc_val: 0.8000 time: 0.0140s\n",
      "Epoch: 0161 loss_train: 0.2449 acc_train: 0.9857 loss_val: 0.6347 acc_val: 0.8033 time: 0.0150s\n",
      "Epoch: 0162 loss_train: 0.2890 acc_train: 0.9429 loss_val: 0.6341 acc_val: 0.8033 time: 0.0150s\n",
      "Epoch: 0163 loss_train: 0.2795 acc_train: 0.9643 loss_val: 0.6327 acc_val: 0.8100 time: 0.0140s\n",
      "Epoch: 0164 loss_train: 0.2972 acc_train: 0.9714 loss_val: 0.6303 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0165 loss_train: 0.3037 acc_train: 0.9643 loss_val: 0.6298 acc_val: 0.8133 time: 0.0153s\n",
      "Epoch: 0166 loss_train: 0.2931 acc_train: 0.9500 loss_val: 0.6311 acc_val: 0.8067 time: 0.0150s\n",
      "Epoch: 0167 loss_train: 0.2596 acc_train: 0.9643 loss_val: 0.6345 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0168 loss_train: 0.2369 acc_train: 0.9857 loss_val: 0.6392 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0169 loss_train: 0.2640 acc_train: 0.9643 loss_val: 0.6415 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0170 loss_train: 0.2786 acc_train: 0.9857 loss_val: 0.6407 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0171 loss_train: 0.2726 acc_train: 0.9643 loss_val: 0.6372 acc_val: 0.8067 time: 0.0150s\n",
      "Epoch: 0172 loss_train: 0.2711 acc_train: 0.9857 loss_val: 0.6302 acc_val: 0.8033 time: 0.0160s\n",
      "Epoch: 0173 loss_train: 0.2265 acc_train: 0.9786 loss_val: 0.6256 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0174 loss_train: 0.3094 acc_train: 0.9643 loss_val: 0.6226 acc_val: 0.8100 time: 0.0130s\n",
      "Epoch: 0175 loss_train: 0.2448 acc_train: 0.9786 loss_val: 0.6225 acc_val: 0.8100 time: 0.0160s\n",
      "Epoch: 0176 loss_train: 0.2433 acc_train: 0.9714 loss_val: 0.6253 acc_val: 0.8033 time: 0.0150s\n",
      "Epoch: 0177 loss_train: 0.2543 acc_train: 0.9786 loss_val: 0.6294 acc_val: 0.8033 time: 0.0150s\n",
      "Epoch: 0178 loss_train: 0.2623 acc_train: 0.9714 loss_val: 0.6326 acc_val: 0.8033 time: 0.0130s\n",
      "Epoch: 0179 loss_train: 0.2462 acc_train: 0.9786 loss_val: 0.6370 acc_val: 0.8033 time: 0.0140s\n",
      "Epoch: 0180 loss_train: 0.2527 acc_train: 0.9643 loss_val: 0.6424 acc_val: 0.8067 time: 0.0140s\n",
      "Epoch: 0181 loss_train: 0.2505 acc_train: 0.9643 loss_val: 0.6455 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0182 loss_train: 0.2484 acc_train: 0.9929 loss_val: 0.6426 acc_val: 0.8067 time: 0.0150s\n",
      "Epoch: 0183 loss_train: 0.2584 acc_train: 0.9714 loss_val: 0.6351 acc_val: 0.8067 time: 0.0130s\n",
      "Epoch: 0184 loss_train: 0.2395 acc_train: 0.9857 loss_val: 0.6285 acc_val: 0.8100 time: 0.0150s\n",
      "Epoch: 0185 loss_train: 0.2833 acc_train: 0.9786 loss_val: 0.6220 acc_val: 0.8100 time: 0.0140s\n",
      "Epoch: 0186 loss_train: 0.2461 acc_train: 0.9643 loss_val: 0.6205 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0187 loss_train: 0.2687 acc_train: 0.9643 loss_val: 0.6220 acc_val: 0.8133 time: 0.0150s\n",
      "Epoch: 0188 loss_train: 0.2625 acc_train: 0.9714 loss_val: 0.6250 acc_val: 0.8100 time: 0.0140s\n",
      "Epoch: 0189 loss_train: 0.2873 acc_train: 0.9786 loss_val: 0.6282 acc_val: 0.8100 time: 0.0141s\n",
      "Epoch: 0190 loss_train: 0.2649 acc_train: 0.9857 loss_val: 0.6331 acc_val: 0.8033 time: 0.0139s\n",
      "Epoch: 0191 loss_train: 0.2450 acc_train: 0.9786 loss_val: 0.6379 acc_val: 0.8067 time: 0.0140s\n",
      "Epoch: 0192 loss_train: 0.2309 acc_train: 0.9857 loss_val: 0.6421 acc_val: 0.8133 time: 0.0140s\n",
      "Epoch: 0193 loss_train: 0.2454 acc_train: 0.9500 loss_val: 0.6417 acc_val: 0.8167 time: 0.0140s\n",
      "Epoch: 0194 loss_train: 0.2724 acc_train: 0.9714 loss_val: 0.6383 acc_val: 0.8200 time: 0.0140s\n",
      "Epoch: 0195 loss_train: 0.2478 acc_train: 0.9786 loss_val: 0.6344 acc_val: 0.8233 time: 0.0160s\n",
      "Epoch: 0196 loss_train: 0.2912 acc_train: 0.9571 loss_val: 0.6279 acc_val: 0.8300 time: 0.0160s\n",
      "Epoch: 0197 loss_train: 0.2683 acc_train: 0.9571 loss_val: 0.6209 acc_val: 0.8233 time: 0.0160s\n",
      "Epoch: 0198 loss_train: 0.2787 acc_train: 0.9571 loss_val: 0.6177 acc_val: 0.8167 time: 0.0170s\n",
      "Epoch: 0199 loss_train: 0.2496 acc_train: 0.9786 loss_val: 0.6173 acc_val: 0.8100 time: 0.0170s\n",
      "Epoch: 0200 loss_train: 0.2471 acc_train: 0.9857 loss_val: 0.6193 acc_val: 0.8100 time: 0.0160s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 3.0472s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.6173 accuracy= 0.8260\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygcn-6VHiZhVm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
